{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression and Support Vector Machines Minilab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are to perform predictive analysis (classification) upon a data set: model the dataset using methods we have discussed in class: logistic regression and support vector machines, and making conclusions from the analysis. Follow the CRISP-DM framework in your analysis (you are not performing all of the CRISP-DM outline, only the portions relevant to the grading rubric outlined below). This report is worth 10% of the final grade. You may complete this assignment in teams of as many as three people.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Martin Garcia, Joanna Duran, Daniel Byrne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to create a logistic regression model and a support vector machine model for the classification of each image. We will determine which is a better model based on prediction accuracy, training time, and efficiency.The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6,000 images per class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to create a logistic regression model and a support vector machine model for the classification of each image in our test set. We will determine which model is best suited for this standard classification task based on a comparison on their prediction accuracy, training times, and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Meaning Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CIFAR-10 dataset\n",
    "\n",
    "We are using the [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The dataset includes are 50000(80%) training images and 10000(20%) test images broken in to 5 pre-randomized training batches and 1 test batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training batch contains 10,000 observations with a row vector of length 3072 representative of color image of 32x32 pixels. The first 1024 columns consist of red values, followed by green, and blue. The data also incorporates labels ranging from 0 to 9 and are listed below.\n",
    "\n",
    "* airplane : 0\n",
    "* automobile : 1\n",
    "* bird : 2\n",
    "* cat : 3\n",
    "* deer : 4\n",
    "* dog : 5\n",
    "* frog : 6\n",
    "* horse : 7\n",
    "* ship : 8\n",
    "* truck : 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test batch contains 1000 randomly-selected images from each class. The 5 training batches are randomized and contain a variable number of images from each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data, reshape to 32x32 matrix per color, transpose matrices\n",
    "def load_cfar10_batch(path, batch_id = None, reshape = True):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    path -- path the datasets\n",
    "    batch_id -- id of the batch (1 to 5) to load\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                    X -- features\n",
    "                    Y -- labels\n",
    "    \"\"\"\n",
    "    if batch_id is not None:\n",
    "        filepath = path + 'data_batch_' + str(batch_id)\n",
    "    else:\n",
    "        filepath = path\n",
    "        \n",
    "    with open(filepath, mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    \n",
    "    if reshape:    \n",
    "        X = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "        Y = np.array(batch['labels'])\n",
    "    else:\n",
    "        X = batch['data']\n",
    "        Y = np.array(batch['labels'])\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch #3:\n",
      "# of Samples: 10000\n",
      "\n",
      "Label Counts of [0](AIRPLANE) : 994\n",
      "Label Counts of [1](AUTOMOBILE) : 1042\n",
      "Label Counts of [2](BIRD) : 965\n",
      "Label Counts of [3](CAT) : 997\n",
      "Label Counts of [4](DEER) : 990\n",
      "Label Counts of [5](DOG) : 1029\n",
      "Label Counts of [6](FROG) : 978\n",
      "Label Counts of [7](HORSE) : 1015\n",
      "Label Counts of [8](SHIP) : 961\n",
      "Label Counts of [9](TRUCK) : 1029\n",
      "\n",
      "Example of Image 4097:\n",
      "Image - Min Value: 44 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load labels for our label\n",
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "#display images\n",
    "def display_stats(data, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(data, batch_id)\n",
    "\n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "\n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "\n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "\n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_image)\n",
    "\n",
    "batch_id = random.randint(1,5)\n",
    "sample_id = random.randint(1,10000)\n",
    "display_stats( \"data/\", batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we will create a logistic regression model and a support vector machine model for the classification task involved with your dataset.  We will assess how well each model performs (use 80/20 training/testing split)and adjust parameters of the models to make them more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (50000, 3072)\n",
      "Labels shape:  (50000,)\n",
      "Test Data shape:  (10000, 3072)\n",
      "Test Labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Build Logistic Regression Model with Stochastic Gradient Descent \n",
    "X,Y = load_cfar10_batch(\"data/\",1,False)\n",
    "\n",
    "for n in range(2,6):\n",
    "    x,y = load_cfar10_batch(\"data/\",n,False)\n",
    "    X = np.concatenate((X,x),axis=0)\n",
    "    Y = np.concatenate((Y,y),axis=0)\n",
    "\n",
    "test_X,test_Y = load_cfar10_batch(\"data/test_batch\",None,False)\n",
    "#labelnames = unpickle(\"data/batches.meta\")\n",
    "\n",
    "sgdlr = SGDClassifier(alpha=0.001, max_iter=10000, tol=1e-3, verbose = 1,n_jobs=4,loss=\"log\")\n",
    "\n",
    "print(\"Data shape: \",X.shape)\n",
    "print(\"Labels shape: \",Y.shape)\n",
    "\n",
    "print(\"Test Data shape: \",test_X.shape)\n",
    "print(\"Test Labels shape: \",test_Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "Norm: 2921.69, NNZs: 3072, Bias: -6.603061, T: 50000, Avg. loss: 423510.589416\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2241.97, NNZs: 3072, Bias: -7.512332, T: 50000, Avg. loss: 475570.319952\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1986.92, NNZs: 3072, Bias: -17.182976, T: 50000, Avg. loss: 475364.134868\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2474.71, NNZs: 3072, Bias: -58.313940, T: 50000, Avg. loss: 559808.534897\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1748.16, NNZs: 3072, Bias: -7.513071, T: 100000, Avg. loss: 52317.068622\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1467.65, NNZs: 3072, Bias: -8.297116, T: 100000, Avg. loss: 62484.280555\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1268.92, NNZs: 3072, Bias: -17.966882, T: 100000, Avg. loss: 62231.211852\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1589.64, NNZs: 3072, Bias: -65.493090, T: 100000, Avg. loss: 71978.223890\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1329.62, NNZs: 3072, Bias: -8.186364, T: 150000, Avg. loss: 30170.991718\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1157.57, NNZs: 3072, Bias: -8.883316, T: 150000, Avg. loss: 36540.331688\n",
      "Total training time: 1.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1018.09, NNZs: 3072, Bias: -18.427341, T: 150000, Avg. loss: 36553.901183\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1262.86, NNZs: 3072, Bias: -69.848637, T: 150000, Avg. loss: 42256.497250\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1092.89, NNZs: 3072, Bias: -8.656652, T: 200000, Avg. loss: 20961.136478\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 983.58, NNZs: 3072, Bias: -9.381773, T: 200000, Avg. loss: 25973.480930\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1040.11, NNZs: 3072, Bias: -72.890895, T: 200000, Avg. loss: 30182.459447\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 874.62, NNZs: 3072, Bias: -18.796476, T: 200000, Avg. loss: 25552.899317\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 943.13, NNZs: 3072, Bias: -8.908338, T: 250000, Avg. loss: 16447.613601\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 915.71, NNZs: 3072, Bias: -75.142361, T: 250000, Avg. loss: 22978.264337\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 856.46, NNZs: 3072, Bias: -9.649378, T: 250000, Avg. loss: 20120.551729\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 743.72, NNZs: 3072, Bias: -19.070894, T: 250000, Avg. loss: 20131.938910\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 859.79, NNZs: 3072, Bias: -9.288462, T: 300000, Avg. loss: 13377.324456\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 831.72, NNZs: 3072, Bias: -77.145214, T: 300000, Avg. loss: 18952.204956\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 678.76, NNZs: 3072, Bias: -19.318726, T: 300000, Avg. loss: 16215.040424\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 767.14, NNZs: 3072, Bias: -9.959944, T: 300000, Avg. loss: 16263.444457\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 792.05, NNZs: 3072, Bias: -9.508871, T: 350000, Avg. loss: 11296.135774\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 756.00, NNZs: 3072, Bias: -78.733361, T: 350000, Avg. loss: 15878.340026\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 632.36, NNZs: 3072, Bias: -19.443938, T: 350000, Avg. loss: 13808.378679\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 697.24, NNZs: 3072, Bias: -10.197862, T: 350000, Avg. loss: 13641.466455\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 733.42, NNZs: 3072, Bias: -9.680350, T: 400000, Avg. loss: 9647.946152\n",
      "Total training time: 3.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 699.92, NNZs: 3072, Bias: -80.186666, T: 400000, Avg. loss: 13727.954133\n",
      "Total training time: 3.25 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 584.96, NNZs: 3072, Bias: -19.636007, T: 400000, Avg. loss: 11790.791003\n",
      "Total training time: 3.25 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 653.67, NNZs: 3072, Bias: -10.342770, T: 400000, Avg. loss: 11995.843569\n",
      "Total training time: 3.26 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 688.36, NNZs: 3072, Bias: -9.822875, T: 450000, Avg. loss: 8600.187734\n",
      "Total training time: 3.60 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 549.90, NNZs: 3072, Bias: -19.797790, T: 450000, Avg. loss: 10516.698262\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 654.75, NNZs: 3072, Bias: -81.422678, T: 450000, Avg. loss: 12125.983190\n",
      "Total training time: 3.70 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 616.46, NNZs: 3072, Bias: -10.516480, T: 450000, Avg. loss: 10461.209771\n",
      "Total training time: 3.73 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 644.49, NNZs: 3072, Bias: -9.967493, T: 500000, Avg. loss: 7625.600919\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 621.39, NNZs: 3072, Bias: -82.529679, T: 500000, Avg. loss: 10842.839235\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 518.63, NNZs: 3072, Bias: -19.988890, T: 500000, Avg. loss: 9254.635414\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 581.53, NNZs: 3072, Bias: -10.661149, T: 500000, Avg. loss: 9312.174583\n",
      "Total training time: 4.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 609.31, NNZs: 3072, Bias: -10.087995, T: 550000, Avg. loss: 6879.360134\n",
      "Total training time: 4.45 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 592.59, NNZs: 3072, Bias: -83.535976, T: 550000, Avg. loss: 9689.335694\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 500.24, NNZs: 3072, Bias: -20.108716, T: 550000, Avg. loss: 8434.753620\n",
      "Total training time: 4.57 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 552.55, NNZs: 3072, Bias: -10.758029, T: 550000, Avg. loss: 8487.759741\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 580.29, NNZs: 3072, Bias: -10.227399, T: 600000, Avg. loss: 6396.478827\n",
      "Total training time: 4.87 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 563.40, NNZs: 3072, Bias: -84.438825, T: 600000, Avg. loss: 8974.105138\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 478.06, NNZs: 3072, Bias: -20.218369, T: 600000, Avg. loss: 7712.267541\n",
      "Total training time: 4.99 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 525.63, NNZs: 3072, Bias: -10.921977, T: 600000, Avg. loss: 7718.162225\n",
      "Total training time: 5.04 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 554.03, NNZs: 3072, Bias: -10.370553, T: 650000, Avg. loss: 5775.555124\n",
      "Total training time: 5.28 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 541.94, NNZs: 3072, Bias: -85.286891, T: 650000, Avg. loss: 8177.296792\n",
      "Total training time: 5.39 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 454.60, NNZs: 3072, Bias: -20.324389, T: 650000, Avg. loss: 7039.924456\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 505.97, NNZs: 3072, Bias: -11.035074, T: 650000, Avg. loss: 7121.017134\n",
      "Total training time: 5.51 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 529.27, NNZs: 3072, Bias: -10.461482, T: 700000, Avg. loss: 5363.381719\n",
      "Total training time: 5.81 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 517.67, NNZs: 3072, Bias: -86.057586, T: 700000, Avg. loss: 7567.253761\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 439.66, NNZs: 3072, Bias: -20.437455, T: 700000, Avg. loss: 6510.306123\n",
      "Total training time: 5.96 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 483.60, NNZs: 3072, Bias: -11.174036, T: 700000, Avg. loss: 6609.014545\n",
      "Total training time: 6.02 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 509.70, NNZs: 3072, Bias: -10.568567, T: 750000, Avg. loss: 4927.850802\n",
      "Total training time: 6.31 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 497.30, NNZs: 3072, Bias: -86.769193, T: 750000, Avg. loss: 7128.049637\n",
      "Total training time: 6.48 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 425.75, NNZs: 3072, Bias: -20.526178, T: 750000, Avg. loss: 6018.525438\n",
      "Total training time: 6.52 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 461.66, NNZs: 3072, Bias: -11.268776, T: 750000, Avg. loss: 6138.510955\n",
      "Total training time: 6.57 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 490.86, NNZs: 3072, Bias: -10.693826, T: 800000, Avg. loss: 4685.487941\n",
      "Total training time: 6.79 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 479.59, NNZs: 3072, Bias: -87.449889, T: 800000, Avg. loss: 6672.566587\n",
      "Total training time: 6.94 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 412.64, NNZs: 3072, Bias: -20.630026, T: 800000, Avg. loss: 5633.186635\n",
      "Total training time: 6.98 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 447.67, NNZs: 3072, Bias: -11.358152, T: 800000, Avg. loss: 5737.810385\n",
      "Total training time: 7.03 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 477.37, NNZs: 3072, Bias: -10.743075, T: 850000, Avg. loss: 4406.769119\n",
      "Total training time: 7.21 seconds.\n",
      "-- Epoch 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 467.02, NNZs: 3072, Bias: -88.104962, T: 850000, Avg. loss: 6177.085402\n",
      "Total training time: 7.40 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 400.73, NNZs: 3072, Bias: -20.734812, T: 850000, Avg. loss: 5349.618966\n",
      "Total training time: 7.42 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 434.49, NNZs: 3072, Bias: -11.443969, T: 850000, Avg. loss: 5377.317781\n",
      "Total training time: 7.51 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 462.16, NNZs: 3072, Bias: -10.796399, T: 900000, Avg. loss: 4074.408533\n",
      "Total training time: 7.66 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 454.27, NNZs: 3072, Bias: -88.693227, T: 900000, Avg. loss: 5865.461898\n",
      "Total training time: 7.88 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 387.48, NNZs: 3072, Bias: -20.825498, T: 900000, Avg. loss: 5013.894797\n",
      "Total training time: 7.90 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 421.23, NNZs: 3072, Bias: -11.489147, T: 900000, Avg. loss: 5062.651923\n",
      "Total training time: 7.97 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 445.33, NNZs: 3072, Bias: -10.869245, T: 950000, Avg. loss: 3920.398990\n",
      "Total training time: 8.10 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 443.53, NNZs: 3072, Bias: -89.239356, T: 950000, Avg. loss: 5561.132987\n",
      "Total training time: 8.32 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 380.02, NNZs: 3072, Bias: -20.873101, T: 950000, Avg. loss: 4770.616303\n",
      "Total training time: 8.34 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 409.10, NNZs: 3072, Bias: -11.581326, T: 950000, Avg. loss: 4793.530818\n",
      "Total training time: 8.41 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 437.55, NNZs: 3072, Bias: -10.954050, T: 1000000, Avg. loss: 3653.483909\n",
      "Total training time: 8.53 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 430.06, NNZs: 3072, Bias: -89.760916, T: 1000000, Avg. loss: 5242.997771\n",
      "Total training time: 8.78 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 370.80, NNZs: 3072, Bias: -20.954062, T: 1000000, Avg. loss: 4519.934605\n",
      "Total training time: 8.80 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 399.63, NNZs: 3072, Bias: -11.680404, T: 1000000, Avg. loss: 4450.445003\n",
      "Total training time: 8.86 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 426.91, NNZs: 3072, Bias: -11.020084, T: 1050000, Avg. loss: 3477.218730\n",
      "Total training time: 8.95 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 419.50, NNZs: 3072, Bias: -90.271886, T: 1050000, Avg. loss: 5016.588622\n",
      "Total training time: 9.19 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 363.12, NNZs: 3072, Bias: -20.995929, T: 1050000, Avg. loss: 4324.694397\n",
      "Total training time: 9.21 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 389.92, NNZs: 3072, Bias: -11.740703, T: 1050000, Avg. loss: 4334.901377\n",
      "Total training time: 9.28 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 416.51, NNZs: 3072, Bias: -11.096255, T: 1100000, Avg. loss: 3312.731979\n",
      "Total training time: 9.35 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 411.90, NNZs: 3072, Bias: -90.752369, T: 1100000, Avg. loss: 4780.322798\n",
      "Total training time: 9.61 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 353.82, NNZs: 3072, Bias: -21.082256, T: 1100000, Avg. loss: 4119.840359\n",
      "Total training time: 9.63 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 379.51, NNZs: 3072, Bias: -11.811879, T: 1100000, Avg. loss: 4096.552404\n",
      "Total training time: 9.74 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 407.32, NNZs: 3072, Bias: -11.133063, T: 1150000, Avg. loss: 3176.232460\n",
      "Total training time: 9.83 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 404.55, NNZs: 3072, Bias: -91.214927, T: 1150000, Avg. loss: 4547.212258\n",
      "Total training time: 10.11 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 344.07, NNZs: 3072, Bias: -21.160014, T: 1150000, Avg. loss: 3909.150339\n",
      "Total training time: 10.14 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 371.81, NNZs: 3072, Bias: -11.873046, T: 1150000, Avg. loss: 3915.958821\n",
      "Total training time: 10.20 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 398.34, NNZs: 3072, Bias: -11.198569, T: 1200000, Avg. loss: 3054.141979\n",
      "Total training time: 10.27 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 395.60, NNZs: 3072, Bias: -91.643865, T: 1200000, Avg. loss: 4313.850450\n",
      "Total training time: 10.55 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 337.06, NNZs: 3072, Bias: -21.222616, T: 1200000, Avg. loss: 3724.086088\n",
      "Total training time: 10.58 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 363.01, NNZs: 3072, Bias: -11.954409, T: 1200000, Avg. loss: 3745.488306\n",
      "Total training time: 10.64 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 390.65, NNZs: 3072, Bias: -11.256870, T: 1250000, Avg. loss: 2887.871289\n",
      "Total training time: 10.69 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 388.67, NNZs: 3072, Bias: -92.058627, T: 1250000, Avg. loss: 4151.510815\n",
      "Total training time: 10.97 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 329.22, NNZs: 3072, Bias: -21.276859, T: 1250000, Avg. loss: 3601.171836\n",
      "Total training time: 11.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 354.53, NNZs: 3072, Bias: -12.004434, T: 1250000, Avg. loss: 3621.766892\n",
      "Total training time: 11.06 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 383.52, NNZs: 3072, Bias: -11.323493, T: 1300000, Avg. loss: 2780.241425\n",
      "Total training time: 11.09 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 382.54, NNZs: 3072, Bias: -92.479574, T: 1300000, Avg. loss: 3998.432137\n",
      "Total training time: 11.39 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 321.73, NNZs: 3072, Bias: -21.309142, T: 1300000, Avg. loss: 3416.618126\n",
      "Total training time: 11.43 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 347.78, NNZs: 3072, Bias: -12.057351, T: 1300000, Avg. loss: 3472.340971\n",
      "Total training time: 11.49 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 374.81, NNZs: 3072, Bias: -11.385214, T: 1350000, Avg. loss: 2706.916814\n",
      "Total training time: 11.50 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 376.76, NNZs: 3072, Bias: -92.865836, T: 1350000, Avg. loss: 3840.506475\n",
      "Total training time: 11.83 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 314.97, NNZs: 3072, Bias: -21.377027, T: 1350000, Avg. loss: 3334.343277\n",
      "Total training time: 11.86 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 339.37, NNZs: 3072, Bias: -12.131896, T: 1350000, Avg. loss: 3338.945108\n",
      "Total training time: 11.92 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 367.36, NNZs: 3072, Bias: -11.440012, T: 1400000, Avg. loss: 2594.083997\n",
      "Total training time: 11.92 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 369.69, NNZs: 3072, Bias: -93.250817, T: 1400000, Avg. loss: 3686.798313\n",
      "Total training time: 12.23 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 310.52, NNZs: 3072, Bias: -21.433250, T: 1400000, Avg. loss: 3195.520041\n",
      "Total training time: 12.27 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 361.50, NNZs: 3072, Bias: -11.499154, T: 1450000, Avg. loss: 2508.435783\n",
      "Total training time: 12.32 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 333.49, NNZs: 3072, Bias: -12.174368, T: 1400000, Avg. loss: 3181.860833\n",
      "Total training time: 12.33 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 363.41, NNZs: 3072, Bias: -93.606942, T: 1450000, Avg. loss: 3543.595643\n",
      "Total training time: 12.64 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 305.57, NNZs: 3072, Bias: -21.487308, T: 1450000, Avg. loss: 3089.059805\n",
      "Total training time: 12.71 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 354.04, NNZs: 3072, Bias: -11.553436, T: 1500000, Avg. loss: 2448.941204\n",
      "Total training time: 12.73 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 328.13, NNZs: 3072, Bias: -12.228766, T: 1450000, Avg. loss: 3110.276014\n",
      "Total training time: 12.74 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 356.72, NNZs: 3072, Bias: -93.943421, T: 1500000, Avg. loss: 3450.088944\n",
      "Total training time: 13.07 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 300.95, NNZs: 3072, Bias: -21.526883, T: 1500000, Avg. loss: 2940.291173\n",
      "Total training time: 13.13 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 348.88, NNZs: 3072, Bias: -11.596800, T: 1550000, Avg. loss: 2366.253823\n",
      "Total training time: 13.15 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 320.48, NNZs: 3072, Bias: -12.296006, T: 1500000, Avg. loss: 3009.176794\n",
      "Total training time: 13.17 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 350.22, NNZs: 3072, Bias: -94.289459, T: 1550000, Avg. loss: 3327.480100\n",
      "Total training time: 13.48 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 343.13, NNZs: 3072, Bias: -11.644926, T: 1600000, Avg. loss: 2249.834173\n",
      "Total training time: 13.55 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 296.04, NNZs: 3072, Bias: -21.572222, T: 1550000, Avg. loss: 2887.961157\n",
      "Total training time: 13.55 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 313.98, NNZs: 3072, Bias: -12.333899, T: 1550000, Avg. loss: 2904.879265\n",
      "Total training time: 13.59 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 344.65, NNZs: 3072, Bias: -94.608401, T: 1600000, Avg. loss: 3217.172160\n",
      "Total training time: 13.90 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 338.72, NNZs: 3072, Bias: -11.696117, T: 1650000, Avg. loss: 2175.507758\n",
      "Total training time: 13.96 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 291.98, NNZs: 3072, Bias: -21.630191, T: 1600000, Avg. loss: 2794.713106\n",
      "Total training time: 13.97 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 309.67, NNZs: 3072, Bias: -12.368910, T: 1600000, Avg. loss: 2765.667534\n",
      "Total training time: 14.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 339.30, NNZs: 3072, Bias: -94.918649, T: 1650000, Avg. loss: 3135.819579\n",
      "Total training time: 14.35 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 332.29, NNZs: 3072, Bias: -11.755316, T: 1700000, Avg. loss: 2122.839324\n",
      "Total training time: 14.41 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 287.45, NNZs: 3072, Bias: -21.685644, T: 1650000, Avg. loss: 2674.918339\n",
      "Total training time: 14.45 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 303.86, NNZs: 3072, Bias: -12.418841, T: 1650000, Avg. loss: 2719.690905\n",
      "Total training time: 14.50 seconds.\n",
      "-- Epoch 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 335.66, NNZs: 3072, Bias: -95.244234, T: 1700000, Avg. loss: 3018.674332\n",
      "Total training time: 14.81 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 328.41, NNZs: 3072, Bias: -11.800862, T: 1750000, Avg. loss: 2058.275077\n",
      "Total training time: 14.83 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 283.01, NNZs: 3072, Bias: -21.735446, T: 1700000, Avg. loss: 2610.447484\n",
      "Total training time: 14.88 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 298.49, NNZs: 3072, Bias: -12.461369, T: 1700000, Avg. loss: 2611.628528\n",
      "Total training time: 14.93 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 330.43, NNZs: 3072, Bias: -95.535018, T: 1750000, Avg. loss: 2919.024538\n",
      "Total training time: 15.24 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 323.50, NNZs: 3072, Bias: -11.849842, T: 1800000, Avg. loss: 2009.233432\n",
      "Total training time: 15.26 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 280.68, NNZs: 3072, Bias: -21.789205, T: 1750000, Avg. loss: 2510.893864\n",
      "Total training time: 15.32 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 294.11, NNZs: 3072, Bias: -12.504939, T: 1750000, Avg. loss: 2553.219433\n",
      "Total training time: 15.36 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 326.62, NNZs: 3072, Bias: -95.825227, T: 1800000, Avg. loss: 2862.858864\n",
      "Total training time: 15.72 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 318.85, NNZs: 3072, Bias: -11.895699, T: 1850000, Avg. loss: 1938.790262\n",
      "Total training time: 15.73 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 276.41, NNZs: 3072, Bias: -21.841149, T: 1800000, Avg. loss: 2468.853986\n",
      "Total training time: 15.81 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 290.27, NNZs: 3072, Bias: -12.549441, T: 1800000, Avg. loss: 2493.395297\n",
      "Total training time: 15.85 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 314.15, NNZs: 3072, Bias: -11.936766, T: 1900000, Avg. loss: 1914.257172\n",
      "Total training time: 16.15 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 321.68, NNZs: 3072, Bias: -96.115201, T: 1850000, Avg. loss: 2791.571418\n",
      "Total training time: 16.15 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 273.14, NNZs: 3072, Bias: -21.877220, T: 1850000, Avg. loss: 2388.737684\n",
      "Total training time: 16.24 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 285.28, NNZs: 3072, Bias: -12.578782, T: 1850000, Avg. loss: 2430.052090\n",
      "Total training time: 16.28 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 309.22, NNZs: 3072, Bias: -11.978112, T: 1950000, Avg. loss: 1878.925563\n",
      "Total training time: 16.55 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 317.61, NNZs: 3072, Bias: -96.379827, T: 1900000, Avg. loss: 2710.746947\n",
      "Total training time: 16.56 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 269.96, NNZs: 3072, Bias: -21.917655, T: 1900000, Avg. loss: 2333.547738\n",
      "Total training time: 16.65 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 281.87, NNZs: 3072, Bias: -12.615179, T: 1900000, Avg. loss: 2356.242821\n",
      "Total training time: 16.70 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 305.08, NNZs: 3072, Bias: -12.015664, T: 2000000, Avg. loss: 1785.708979\n",
      "Total training time: 16.96 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 313.17, NNZs: 3072, Bias: -96.657213, T: 1950000, Avg. loss: 2610.929745\n",
      "Total training time: 16.98 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 266.36, NNZs: 3072, Bias: -21.955311, T: 1950000, Avg. loss: 2287.131393\n",
      "Total training time: 17.09 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 277.92, NNZs: 3072, Bias: -12.655219, T: 1950000, Avg. loss: 2316.529144\n",
      "Total training time: 17.12 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 300.93, NNZs: 3072, Bias: -12.055729, T: 2050000, Avg. loss: 1747.582028\n",
      "Total training time: 17.36 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 308.69, NNZs: 3072, Bias: -96.909527, T: 2000000, Avg. loss: 2538.731133\n",
      "Total training time: 17.40 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 262.80, NNZs: 3072, Bias: -21.979377, T: 2000000, Avg. loss: 2210.633730\n",
      "Total training time: 17.50 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 274.71, NNZs: 3072, Bias: -12.695057, T: 2000000, Avg. loss: 2208.283091\n",
      "Total training time: 17.54 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 297.44, NNZs: 3072, Bias: -12.100066, T: 2100000, Avg. loss: 1713.750110\n",
      "Total training time: 17.79 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 304.47, NNZs: 3072, Bias: -97.163132, T: 2050000, Avg. loss: 2503.988969\n",
      "Total training time: 17.84 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 258.94, NNZs: 3072, Bias: -22.010628, T: 2050000, Avg. loss: 2163.696819\n",
      "Total training time: 17.94 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 271.45, NNZs: 3072, Bias: -12.739987, T: 2050000, Avg. loss: 2158.501178\n",
      "Total training time: 17.97 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 294.81, NNZs: 3072, Bias: -12.135559, T: 2150000, Avg. loss: 1656.067697\n",
      "Total training time: 18.21 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 301.24, NNZs: 3072, Bias: -97.402132, T: 2100000, Avg. loss: 2417.547339\n",
      "Total training time: 18.27 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 256.01, NNZs: 3072, Bias: -22.037992, T: 2100000, Avg. loss: 2116.413992\n",
      "Total training time: 18.37 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 267.92, NNZs: 3072, Bias: -12.773573, T: 2100000, Avg. loss: 2107.462660\n",
      "Total training time: 18.41 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 290.42, NNZs: 3072, Bias: -12.174578, T: 2200000, Avg. loss: 1625.363774\n",
      "Total training time: 18.63 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 297.20, NNZs: 3072, Bias: -97.654597, T: 2150000, Avg. loss: 2381.604544\n",
      "Total training time: 18.71 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 253.80, NNZs: 3072, Bias: -22.069746, T: 2150000, Avg. loss: 2056.841690\n",
      "Total training time: 18.82 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 265.08, NNZs: 3072, Bias: -12.804578, T: 2150000, Avg. loss: 2080.016847\n",
      "Total training time: 18.86 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 287.45, NNZs: 3072, Bias: -12.215134, T: 2250000, Avg. loss: 1580.446309\n",
      "Total training time: 19.06 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 293.58, NNZs: 3072, Bias: -97.896397, T: 2200000, Avg. loss: 2308.513977\n",
      "Total training time: 19.14 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 250.70, NNZs: 3072, Bias: -22.096899, T: 2200000, Avg. loss: 2015.274995\n",
      "Total training time: 19.25 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 262.42, NNZs: 3072, Bias: -12.841815, T: 2200000, Avg. loss: 2037.917575\n",
      "Total training time: 19.29 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 284.20, NNZs: 3072, Bias: -12.237476, T: 2300000, Avg. loss: 1559.239383\n",
      "Total training time: 19.47 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 290.36, NNZs: 3072, Bias: -98.126087, T: 2250000, Avg. loss: 2250.261824\n",
      "Total training time: 19.57 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 247.40, NNZs: 3072, Bias: -22.136411, T: 2250000, Avg. loss: 1970.161734\n",
      "Total training time: 19.70 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 260.03, NNZs: 3072, Bias: -12.876308, T: 2250000, Avg. loss: 1955.243346\n",
      "Total training time: 19.73 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 281.68, NNZs: 3072, Bias: -12.269679, T: 2350000, Avg. loss: 1513.501599\n",
      "Total training time: 19.90 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 287.74, NNZs: 3072, Bias: -98.351616, T: 2300000, Avg. loss: 2199.514915\n",
      "Total training time: 20.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 245.61, NNZs: 3072, Bias: -22.157534, T: 2300000, Avg. loss: 1883.855328\n",
      "Total training time: 20.14 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 256.63, NNZs: 3072, Bias: -12.920458, T: 2300000, Avg. loss: 1920.843156\n",
      "Total training time: 20.16 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 278.82, NNZs: 3072, Bias: -12.297734, T: 2400000, Avg. loss: 1486.852265\n",
      "Total training time: 20.32 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 284.44, NNZs: 3072, Bias: -98.577981, T: 2350000, Avg. loss: 2188.318208\n",
      "Total training time: 20.44 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 243.36, NNZs: 3072, Bias: -22.195790, T: 2350000, Avg. loss: 1867.321142\n",
      "Total training time: 20.58 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 253.52, NNZs: 3072, Bias: -12.955948, T: 2350000, Avg. loss: 1868.399317\n",
      "Total training time: 20.59 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 276.19, NNZs: 3072, Bias: -12.318565, T: 2450000, Avg. loss: 1469.631100\n",
      "Total training time: 20.74 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 281.98, NNZs: 3072, Bias: -98.801578, T: 2400000, Avg. loss: 2131.002195\n",
      "Total training time: 20.87 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 241.02, NNZs: 3072, Bias: -22.228046, T: 2400000, Avg. loss: 1834.249193\n",
      "Total training time: 21.02 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 250.68, NNZs: 3072, Bias: -12.983745, T: 2400000, Avg. loss: 1858.335643\n",
      "Total training time: 21.02 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 273.15, NNZs: 3072, Bias: -12.351654, T: 2500000, Avg. loss: 1410.980750\n",
      "Total training time: 21.15 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 279.38, NNZs: 3072, Bias: -99.018599, T: 2450000, Avg. loss: 2083.958955\n",
      "Total training time: 21.28 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 238.68, NNZs: 3072, Bias: -22.256157, T: 2450000, Avg. loss: 1801.210089\n",
      "Total training time: 21.44 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 248.20, NNZs: 3072, Bias: -13.013109, T: 2450000, Avg. loss: 1819.128708\n",
      "Total training time: 21.45 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 269.98, NNZs: 3072, Bias: -12.376910, T: 2550000, Avg. loss: 1378.327803\n",
      "Total training time: 21.55 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 276.91, NNZs: 3072, Bias: -99.231818, T: 2500000, Avg. loss: 2017.456256\n",
      "Total training time: 21.71 seconds.\n",
      "-- Epoch 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 245.46, NNZs: 3072, Bias: -13.047438, T: 2500000, Avg. loss: 1754.947650\n",
      "Total training time: 21.89 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 235.98, NNZs: 3072, Bias: -22.287912, T: 2500000, Avg. loss: 1760.495537\n",
      "Total training time: 21.89 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 267.81, NNZs: 3072, Bias: -12.396456, T: 2600000, Avg. loss: 1367.038744\n",
      "Total training time: 21.98 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 274.70, NNZs: 3072, Bias: -99.440589, T: 2550000, Avg. loss: 1981.364762\n",
      "Total training time: 22.14 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 232.90, NNZs: 3072, Bias: -22.314657, T: 2550000, Avg. loss: 1732.936362\n",
      "Total training time: 22.30 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 243.33, NNZs: 3072, Bias: -13.080858, T: 2550000, Avg. loss: 1760.427380\n",
      "Total training time: 22.31 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 264.40, NNZs: 3072, Bias: -12.428118, T: 2650000, Avg. loss: 1358.034808\n",
      "Total training time: 22.39 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 271.43, NNZs: 3072, Bias: -99.645257, T: 2600000, Avg. loss: 1961.492455\n",
      "Total training time: 22.57 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 241.08, NNZs: 3072, Bias: -13.111663, T: 2600000, Avg. loss: 1709.922977\n",
      "Total training time: 22.75 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 230.78, NNZs: 3072, Bias: -22.346047, T: 2600000, Avg. loss: 1678.398944\n",
      "Total training time: 22.76 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 261.94, NNZs: 3072, Bias: -12.450775, T: 2700000, Avg. loss: 1313.492426\n",
      "Total training time: 22.82 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 268.50, NNZs: 3072, Bias: -99.850621, T: 2650000, Avg. loss: 1907.711625\n",
      "Total training time: 23.01 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 228.82, NNZs: 3072, Bias: -22.373441, T: 2650000, Avg. loss: 1644.769185\n",
      "Total training time: 23.18 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 238.98, NNZs: 3072, Bias: -13.139013, T: 2650000, Avg. loss: 1662.917143\n",
      "Total training time: 23.19 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 259.02, NNZs: 3072, Bias: -12.482645, T: 2750000, Avg. loss: 1302.599838\n",
      "Total training time: 23.23 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 266.18, NNZs: 3072, Bias: -100.052202, T: 2700000, Avg. loss: 1888.743547\n",
      "Total training time: 23.43 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 235.80, NNZs: 3072, Bias: -13.166556, T: 2700000, Avg. loss: 1655.830804\n",
      "Total training time: 23.63 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 226.67, NNZs: 3072, Bias: -22.400493, T: 2700000, Avg. loss: 1642.503043\n",
      "Total training time: 23.62 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 257.14, NNZs: 3072, Bias: -12.512114, T: 2800000, Avg. loss: 1233.284871\n",
      "Total training time: 23.66 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 263.55, NNZs: 3072, Bias: -100.227421, T: 2750000, Avg. loss: 1847.161794\n",
      "Total training time: 23.87 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 225.47, NNZs: 3072, Bias: -22.420505, T: 2750000, Avg. loss: 1603.170793\n",
      "Total training time: 24.06 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 233.14, NNZs: 3072, Bias: -13.195257, T: 2750000, Avg. loss: 1605.892263\n",
      "Total training time: 24.07 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 254.80, NNZs: 3072, Bias: -12.541479, T: 2850000, Avg. loss: 1247.193379\n",
      "Total training time: 24.08 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 261.04, NNZs: 3072, Bias: -100.415532, T: 2800000, Avg. loss: 1808.106081\n",
      "Total training time: 24.29 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 222.80, NNZs: 3072, Bias: -22.455301, T: 2800000, Avg. loss: 1586.462591\n",
      "Total training time: 24.49 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 252.75, NNZs: 3072, Bias: -12.560466, T: 2900000, Avg. loss: 1219.390026\n",
      "Total training time: 24.50 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 231.08, NNZs: 3072, Bias: -13.221122, T: 2800000, Avg. loss: 1557.555687\n",
      "Total training time: 24.52 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 258.79, NNZs: 3072, Bias: -100.599110, T: 2850000, Avg. loss: 1781.658472\n",
      "Total training time: 24.73 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 250.66, NNZs: 3072, Bias: -12.592687, T: 2950000, Avg. loss: 1186.892285\n",
      "Total training time: 24.92 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 220.80, NNZs: 3072, Bias: -22.486213, T: 2850000, Avg. loss: 1538.253177\n",
      "Total training time: 24.93 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 228.47, NNZs: 3072, Bias: -13.248825, T: 2850000, Avg. loss: 1552.348501\n",
      "Total training time: 24.96 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 256.23, NNZs: 3072, Bias: -100.773283, T: 2900000, Avg. loss: 1737.701741\n",
      "Total training time: 25.17 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 248.58, NNZs: 3072, Bias: -12.617011, T: 3000000, Avg. loss: 1192.881518\n",
      "Total training time: 25.34 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 218.66, NNZs: 3072, Bias: -22.500006, T: 2900000, Avg. loss: 1534.914704\n",
      "Total training time: 25.37 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 226.59, NNZs: 3072, Bias: -13.274539, T: 2900000, Avg. loss: 1523.900330\n",
      "Total training time: 25.39 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 254.22, NNZs: 3072, Bias: -100.952004, T: 2950000, Avg. loss: 1722.525945\n",
      "Total training time: 25.60 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 246.80, NNZs: 3072, Bias: -12.635630, T: 3050000, Avg. loss: 1152.173670\n",
      "Total training time: 25.78 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 216.69, NNZs: 3072, Bias: -22.528478, T: 2950000, Avg. loss: 1494.745521\n",
      "Total training time: 25.82 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 225.11, NNZs: 3072, Bias: -13.297342, T: 2950000, Avg. loss: 1477.091897\n",
      "Total training time: 25.84 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 252.35, NNZs: 3072, Bias: -101.132627, T: 3000000, Avg. loss: 1677.799260\n",
      "Total training time: 26.03 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 244.48, NNZs: 3072, Bias: -12.664086, T: 3100000, Avg. loss: 1144.958379\n",
      "Total training time: 26.20 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 215.30, NNZs: 3072, Bias: -22.555956, T: 3000000, Avg. loss: 1439.485969\n",
      "Total training time: 26.26 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 223.06, NNZs: 3072, Bias: -13.328655, T: 3000000, Avg. loss: 1473.659021\n",
      "Total training time: 26.27 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 250.34, NNZs: 3072, Bias: -101.300268, T: 3050000, Avg. loss: 1676.905629\n",
      "Total training time: 26.47 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 242.68, NNZs: 3072, Bias: -12.683207, T: 3150000, Avg. loss: 1116.187197\n",
      "Total training time: 26.61 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 213.53, NNZs: 3072, Bias: -22.584315, T: 3050000, Avg. loss: 1430.487691\n",
      "Total training time: 26.70 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 220.65, NNZs: 3072, Bias: -13.352033, T: 3050000, Avg. loss: 1447.432767\n",
      "Total training time: 26.71 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 248.37, NNZs: 3072, Bias: -101.472111, T: 3100000, Avg. loss: 1630.764809\n",
      "Total training time: 26.90 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 240.72, NNZs: 3072, Bias: -12.708995, T: 3200000, Avg. loss: 1106.949983\n",
      "Total training time: 27.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 212.07, NNZs: 3072, Bias: -22.610628, T: 3100000, Avg. loss: 1416.325542\n",
      "Total training time: 27.14 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 218.82, NNZs: 3072, Bias: -13.380928, T: 3100000, Avg. loss: 1424.828470\n",
      "Total training time: 27.15 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 246.12, NNZs: 3072, Bias: -101.634442, T: 3150000, Avg. loss: 1619.995111\n",
      "Total training time: 27.33 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 239.17, NNZs: 3072, Bias: -12.730996, T: 3250000, Avg. loss: 1087.422495\n",
      "Total training time: 27.47 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 209.76, NNZs: 3072, Bias: -22.635168, T: 3150000, Avg. loss: 1394.387541\n",
      "Total training time: 27.58 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 217.02, NNZs: 3072, Bias: -13.406828, T: 3150000, Avg. loss: 1406.996201\n",
      "Total training time: 27.59 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 243.30, NNZs: 3072, Bias: -101.803698, T: 3200000, Avg. loss: 1589.668580\n",
      "Total training time: 27.79 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 237.29, NNZs: 3072, Bias: -12.751787, T: 3300000, Avg. loss: 1063.023488\n",
      "Total training time: 27.92 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 208.23, NNZs: 3072, Bias: -22.656112, T: 3200000, Avg. loss: 1372.627186\n",
      "Total training time: 28.04 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 215.20, NNZs: 3072, Bias: -13.427234, T: 3200000, Avg. loss: 1362.648941\n",
      "Total training time: 28.06 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 241.25, NNZs: 3072, Bias: -101.965420, T: 3250000, Avg. loss: 1558.371734\n",
      "Total training time: 28.23 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 235.49, NNZs: 3072, Bias: -12.774450, T: 3350000, Avg. loss: 1055.700867\n",
      "Total training time: 28.36 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 206.76, NNZs: 3072, Bias: -22.673569, T: 3250000, Avg. loss: 1338.100887\n",
      "Total training time: 28.52 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 213.37, NNZs: 3072, Bias: -13.452739, T: 3250000, Avg. loss: 1345.527980\n",
      "Total training time: 28.53 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 239.35, NNZs: 3072, Bias: -102.126246, T: 3300000, Avg. loss: 1543.638805\n",
      "Total training time: 28.70 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 234.12, NNZs: 3072, Bias: -12.800895, T: 3400000, Avg. loss: 1032.222753\n",
      "Total training time: 28.81 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 205.63, NNZs: 3072, Bias: -22.703666, T: 3300000, Avg. loss: 1316.302790\n",
      "Total training time: 28.96 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 212.01, NNZs: 3072, Bias: -13.477397, T: 3300000, Avg. loss: 1325.885542\n",
      "Total training time: 28.98 seconds.\n",
      "-- Epoch 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 237.71, NNZs: 3072, Bias: -102.290070, T: 3350000, Avg. loss: 1529.074640\n",
      "Total training time: 29.13 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 232.77, NNZs: 3072, Bias: -12.821575, T: 3450000, Avg. loss: 1025.817473\n",
      "Total training time: 29.24 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 204.06, NNZs: 3072, Bias: -22.724664, T: 3350000, Avg. loss: 1311.127351\n",
      "Total training time: 29.39 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 210.58, NNZs: 3072, Bias: -13.488119, T: 3350000, Avg. loss: 1312.256032\n",
      "Total training time: 29.41 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 235.29, NNZs: 3072, Bias: -102.442459, T: 3400000, Avg. loss: 1498.777714\n",
      "Total training time: 29.55 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 230.34, NNZs: 3072, Bias: -12.847999, T: 3500000, Avg. loss: 1011.767326\n",
      "Total training time: 29.66 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 202.64, NNZs: 3072, Bias: -22.751404, T: 3400000, Avg. loss: 1310.822304\n",
      "Total training time: 29.83 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 209.09, NNZs: 3072, Bias: -13.513150, T: 3400000, Avg. loss: 1287.383352\n",
      "Total training time: 29.84 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 233.37, NNZs: 3072, Bias: -102.583501, T: 3450000, Avg. loss: 1455.925027\n",
      "Total training time: 29.98 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 229.00, NNZs: 3072, Bias: -12.872592, T: 3550000, Avg. loss: 989.106114\n",
      "Total training time: 30.08 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 201.13, NNZs: 3072, Bias: -22.769921, T: 3450000, Avg. loss: 1271.057082\n",
      "Total training time: 30.26 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 207.54, NNZs: 3072, Bias: -13.532707, T: 3450000, Avg. loss: 1269.230315\n",
      "Total training time: 30.27 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 231.90, NNZs: 3072, Bias: -102.729464, T: 3500000, Avg. loss: 1426.611226\n",
      "Total training time: 30.40 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 227.35, NNZs: 3072, Bias: -12.894321, T: 3600000, Avg. loss: 979.088842\n",
      "Total training time: 30.49 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 199.19, NNZs: 3072, Bias: -22.790662, T: 3500000, Avg. loss: 1251.288385\n",
      "Total training time: 30.70 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 205.83, NNZs: 3072, Bias: -13.555952, T: 3500000, Avg. loss: 1260.407441\n",
      "Total training time: 30.72 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 230.09, NNZs: 3072, Bias: -102.880509, T: 3550000, Avg. loss: 1409.913482\n",
      "Total training time: 30.84 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 225.63, NNZs: 3072, Bias: -12.920137, T: 3650000, Avg. loss: 965.429108\n",
      "Total training time: 30.91 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 198.24, NNZs: 3072, Bias: -22.809464, T: 3550000, Avg. loss: 1232.196983\n",
      "Total training time: 31.13 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 204.74, NNZs: 3072, Bias: -13.578765, T: 3550000, Avg. loss: 1241.902959\n",
      "Total training time: 31.15 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 229.05, NNZs: 3072, Bias: -103.028538, T: 3600000, Avg. loss: 1417.186191\n",
      "Total training time: 31.27 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 224.37, NNZs: 3072, Bias: -12.940386, T: 3700000, Avg. loss: 959.557546\n",
      "Total training time: 31.33 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 197.10, NNZs: 3072, Bias: -22.826489, T: 3600000, Avg. loss: 1213.742756\n",
      "Total training time: 31.58 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 203.55, NNZs: 3072, Bias: -13.600923, T: 3600000, Avg. loss: 1226.118473\n",
      "Total training time: 31.61 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 227.13, NNZs: 3072, Bias: -103.175543, T: 3650000, Avg. loss: 1396.379655\n",
      "Total training time: 31.73 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 222.91, NNZs: 3072, Bias: -12.960157, T: 3750000, Avg. loss: 942.625165\n",
      "Total training time: 31.78 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 195.70, NNZs: 3072, Bias: -22.850452, T: 3650000, Avg. loss: 1192.444921\n",
      "Total training time: 32.01 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 201.73, NNZs: 3072, Bias: -13.632614, T: 3650000, Avg. loss: 1200.084119\n",
      "Total training time: 32.06 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 225.81, NNZs: 3072, Bias: -103.318307, T: 3700000, Avg. loss: 1368.589808\n",
      "Total training time: 32.16 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 221.07, NNZs: 3072, Bias: -12.978798, T: 3800000, Avg. loss: 919.916059\n",
      "Total training time: 32.19 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 194.36, NNZs: 3072, Bias: -22.866628, T: 3700000, Avg. loss: 1182.399081\n",
      "Total training time: 32.43 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 200.30, NNZs: 3072, Bias: -13.653299, T: 3700000, Avg. loss: 1185.448621\n",
      "Total training time: 32.48 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 224.81, NNZs: 3072, Bias: -103.456369, T: 3750000, Avg. loss: 1366.263246\n",
      "Total training time: 32.59 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 219.70, NNZs: 3072, Bias: -12.997456, T: 3850000, Avg. loss: 922.814142\n",
      "Total training time: 32.62 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 193.18, NNZs: 3072, Bias: -22.884354, T: 3750000, Avg. loss: 1169.440677\n",
      "Total training time: 32.91 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 199.38, NNZs: 3072, Bias: -13.672001, T: 3750000, Avg. loss: 1162.671795\n",
      "Total training time: 32.98 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 223.42, NNZs: 3072, Bias: -103.602455, T: 3800000, Avg. loss: 1327.664496\n",
      "Total training time: 33.12 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 218.43, NNZs: 3072, Bias: -13.016916, T: 3900000, Avg. loss: 901.859272\n",
      "Total training time: 33.14 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 191.66, NNZs: 3072, Bias: -22.898894, T: 3800000, Avg. loss: 1156.915036\n",
      "Total training time: 33.44 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 198.10, NNZs: 3072, Bias: -13.692277, T: 3800000, Avg. loss: 1160.914266\n",
      "Total training time: 33.51 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 222.08, NNZs: 3072, Bias: -103.730634, T: 3850000, Avg. loss: 1315.667739\n",
      "Total training time: 33.60 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 217.00, NNZs: 3072, Bias: -13.036411, T: 3950000, Avg. loss: 887.049260\n",
      "Total training time: 33.61 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 190.66, NNZs: 3072, Bias: -22.919502, T: 3850000, Avg. loss: 1134.671708\n",
      "Total training time: 33.92 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 197.13, NNZs: 3072, Bias: -13.712536, T: 3850000, Avg. loss: 1131.960563\n",
      "Total training time: 33.99 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 215.20, NNZs: 3072, Bias: -13.054883, T: 4000000, Avg. loss: 888.305146\n",
      "Total training time: 34.08 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 220.51, NNZs: 3072, Bias: -103.867924, T: 3900000, Avg. loss: 1297.820983\n",
      "Total training time: 34.07 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 189.54, NNZs: 3072, Bias: -22.936966, T: 3900000, Avg. loss: 1131.407951\n",
      "Total training time: 34.37 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 195.45, NNZs: 3072, Bias: -13.723551, T: 3900000, Avg. loss: 1131.547028\n",
      "Total training time: 34.44 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 213.89, NNZs: 3072, Bias: -13.075627, T: 4050000, Avg. loss: 873.113785\n",
      "Total training time: 34.51 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 218.83, NNZs: 3072, Bias: -104.001432, T: 3950000, Avg. loss: 1276.485033\n",
      "Total training time: 34.52 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 188.11, NNZs: 3072, Bias: -22.952497, T: 3950000, Avg. loss: 1109.281898\n",
      "Total training time: 34.85 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 193.77, NNZs: 3072, Bias: -13.745768, T: 3950000, Avg. loss: 1113.980932\n",
      "Total training time: 34.91 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 212.79, NNZs: 3072, Bias: -13.095705, T: 4100000, Avg. loss: 863.401687\n",
      "Total training time: 34.96 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 217.22, NNZs: 3072, Bias: -104.129428, T: 4000000, Avg. loss: 1271.174528\n",
      "Total training time: 34.98 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 186.46, NNZs: 3072, Bias: -22.976100, T: 4000000, Avg. loss: 1103.698820\n",
      "Total training time: 35.34 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 192.65, NNZs: 3072, Bias: -13.762522, T: 4000000, Avg. loss: 1097.702820\n",
      "Total training time: 35.40 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 211.49, NNZs: 3072, Bias: -13.119240, T: 4150000, Avg. loss: 846.147273\n",
      "Total training time: 35.43 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 215.67, NNZs: 3072, Bias: -104.256766, T: 4050000, Avg. loss: 1240.456295\n",
      "Total training time: 35.47 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 185.38, NNZs: 3072, Bias: -22.993486, T: 4050000, Avg. loss: 1090.945140\n",
      "Total training time: 35.81 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 191.31, NNZs: 3072, Bias: -13.780680, T: 4050000, Avg. loss: 1086.852237\n",
      "Total training time: 35.86 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 210.14, NNZs: 3072, Bias: -13.136412, T: 4200000, Avg. loss: 834.983345\n",
      "Total training time: 35.87 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 214.44, NNZs: 3072, Bias: -104.383818, T: 4100000, Avg. loss: 1225.348850\n",
      "Total training time: 35.91 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 184.36, NNZs: 3072, Bias: -23.019550, T: 4100000, Avg. loss: 1068.894820\n",
      "Total training time: 36.25 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 209.04, NNZs: 3072, Bias: -13.157173, T: 4250000, Avg. loss: 822.993374\n",
      "Total training time: 36.29 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 189.93, NNZs: 3072, Bias: -13.803188, T: 4100000, Avg. loss: 1073.867955\n",
      "Total training time: 36.29 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 212.95, NNZs: 3072, Bias: -104.512980, T: 4150000, Avg. loss: 1217.336948\n",
      "Total training time: 36.34 seconds.\n",
      "-- Epoch 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 183.14, NNZs: 3072, Bias: -23.038093, T: 4150000, Avg. loss: 1055.516272\n",
      "Total training time: 36.69 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 207.80, NNZs: 3072, Bias: -13.175970, T: 4300000, Avg. loss: 811.460302\n",
      "Total training time: 36.70 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 188.97, NNZs: 3072, Bias: -13.822734, T: 4150000, Avg. loss: 1064.432446\n",
      "Total training time: 36.72 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 211.62, NNZs: 3072, Bias: -104.636255, T: 4200000, Avg. loss: 1201.795927\n",
      "Total training time: 36.78 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 206.66, NNZs: 3072, Bias: -13.197755, T: 4350000, Avg. loss: 800.642067\n",
      "Total training time: 37.14 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 182.35, NNZs: 3072, Bias: -23.054636, T: 4200000, Avg. loss: 1040.120300\n",
      "Total training time: 37.14 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 188.23, NNZs: 3072, Bias: -13.841845, T: 4200000, Avg. loss: 1039.361511\n",
      "Total training time: 37.16 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 210.36, NNZs: 3072, Bias: -104.748164, T: 4250000, Avg. loss: 1180.649104\n",
      "Total training time: 37.22 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 205.24, NNZs: 3072, Bias: -13.218812, T: 4400000, Avg. loss: 799.372288\n",
      "Total training time: 37.55 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 181.22, NNZs: 3072, Bias: -23.071835, T: 4250000, Avg. loss: 1038.151324\n",
      "Total training time: 37.57 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 186.60, NNZs: 3072, Bias: -13.858013, T: 4250000, Avg. loss: 1036.453403\n",
      "Total training time: 37.60 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 209.51, NNZs: 3072, Bias: -104.865337, T: 4300000, Avg. loss: 1181.557219\n",
      "Total training time: 37.65 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 203.85, NNZs: 3072, Bias: -13.240427, T: 4450000, Avg. loss: 785.660502\n",
      "Total training time: 37.97 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 180.05, NNZs: 3072, Bias: -23.090785, T: 4300000, Avg. loss: 1010.551439\n",
      "Total training time: 38.01 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 185.54, NNZs: 3072, Bias: -13.869077, T: 4300000, Avg. loss: 1015.070212\n",
      "Total training time: 38.04 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 208.34, NNZs: 3072, Bias: -104.981532, T: 4350000, Avg. loss: 1144.229121\n",
      "Total training time: 38.08 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 202.55, NNZs: 3072, Bias: -13.255594, T: 4500000, Avg. loss: 782.324228\n",
      "Total training time: 38.43 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 178.99, NNZs: 3072, Bias: -23.107493, T: 4350000, Avg. loss: 1008.389592\n",
      "Total training time: 38.47 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 184.44, NNZs: 3072, Bias: -13.881714, T: 4350000, Avg. loss: 1014.566927\n",
      "Total training time: 38.50 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 207.39, NNZs: 3072, Bias: -105.104196, T: 4400000, Avg. loss: 1140.505642\n",
      "Total training time: 38.53 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 201.47, NNZs: 3072, Bias: -13.267810, T: 4550000, Avg. loss: 766.895705\n",
      "Total training time: 38.87 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 177.90, NNZs: 3072, Bias: -23.127760, T: 4400000, Avg. loss: 994.980522\n",
      "Total training time: 38.92 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 183.25, NNZs: 3072, Bias: -13.897267, T: 4400000, Avg. loss: 997.962743\n",
      "Total training time: 38.95 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 206.27, NNZs: 3072, Bias: -105.218904, T: 4450000, Avg. loss: 1131.808880\n",
      "Total training time: 38.97 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 200.20, NNZs: 3072, Bias: -13.283679, T: 4600000, Avg. loss: 764.319818\n",
      "Total training time: 39.28 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 176.88, NNZs: 3072, Bias: -23.143614, T: 4450000, Avg. loss: 975.606668\n",
      "Total training time: 39.35 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 182.25, NNZs: 3072, Bias: -13.911210, T: 4450000, Avg. loss: 991.566836\n",
      "Total training time: 39.38 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 204.94, NNZs: 3072, Bias: -105.335928, T: 4500000, Avg. loss: 1128.776922\n",
      "Total training time: 39.40 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 199.15, NNZs: 3072, Bias: -13.296873, T: 4650000, Avg. loss: 754.682363\n",
      "Total training time: 39.73 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 175.88, NNZs: 3072, Bias: -23.160995, T: 4500000, Avg. loss: 976.730246\n",
      "Total training time: 39.82 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 203.91, NNZs: 3072, Bias: -105.452332, T: 4550000, Avg. loss: 1107.574582\n",
      "Total training time: 39.86 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 181.32, NNZs: 3072, Bias: -13.925006, T: 4500000, Avg. loss: 967.337170\n",
      "Total training time: 39.87 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 197.97, NNZs: 3072, Bias: -13.316056, T: 4700000, Avg. loss: 753.568727\n",
      "Total training time: 40.15 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 175.23, NNZs: 3072, Bias: -23.178735, T: 4550000, Avg. loss: 951.195426\n",
      "Total training time: 40.25 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 202.97, NNZs: 3072, Bias: -105.569289, T: 4600000, Avg. loss: 1092.033292\n",
      "Total training time: 40.29 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 180.29, NNZs: 3072, Bias: -13.946098, T: 4550000, Avg. loss: 964.190055\n",
      "Total training time: 40.30 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 196.81, NNZs: 3072, Bias: -13.334208, T: 4750000, Avg. loss: 742.440091\n",
      "Total training time: 40.57 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 174.25, NNZs: 3072, Bias: -23.197677, T: 4600000, Avg. loss: 938.461826\n",
      "Total training time: 40.74 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 201.85, NNZs: 3072, Bias: -105.680396, T: 4650000, Avg. loss: 1101.674057\n",
      "Total training time: 40.76 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 179.31, NNZs: 3072, Bias: -13.965150, T: 4600000, Avg. loss: 949.016818\n",
      "Total training time: 40.77 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 195.80, NNZs: 3072, Bias: -13.350701, T: 4800000, Avg. loss: 734.130054\n",
      "Total training time: 41.04 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 173.43, NNZs: 3072, Bias: -23.218246, T: 4650000, Avg. loss: 934.477737\n",
      "Total training time: 41.18 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 200.66, NNZs: 3072, Bias: -105.792835, T: 4700000, Avg. loss: 1079.037325\n",
      "Total training time: 41.20 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 178.38, NNZs: 3072, Bias: -13.978310, T: 4650000, Avg. loss: 946.126531\n",
      "Total training time: 41.21 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 195.14, NNZs: 3072, Bias: -13.368431, T: 4850000, Avg. loss: 716.438775\n",
      "Total training time: 41.47 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 172.71, NNZs: 3072, Bias: -23.232416, T: 4700000, Avg. loss: 930.397412\n",
      "Total training time: 41.64 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 199.56, NNZs: 3072, Bias: -105.900445, T: 4750000, Avg. loss: 1046.514108\n",
      "Total training time: 41.65 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 177.46, NNZs: 3072, Bias: -13.994306, T: 4700000, Avg. loss: 934.980469\n",
      "Total training time: 41.66 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 194.11, NNZs: 3072, Bias: -13.386214, T: 4900000, Avg. loss: 726.813262\n",
      "Total training time: 41.95 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 198.34, NNZs: 3072, Bias: -106.012715, T: 4800000, Avg. loss: 1062.692280\n",
      "Total training time: 42.12 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 171.82, NNZs: 3072, Bias: -23.248302, T: 4750000, Avg. loss: 912.029969\n",
      "Total training time: 42.14 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 176.48, NNZs: 3072, Bias: -14.008198, T: 4750000, Avg. loss: 913.196267\n",
      "Total training time: 42.14 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 192.93, NNZs: 3072, Bias: -13.401626, T: 4950000, Avg. loss: 708.856076\n",
      "Total training time: 42.36 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 197.08, NNZs: 3072, Bias: -106.121290, T: 4850000, Avg. loss: 1037.191342\n",
      "Total training time: 42.59 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 170.68, NNZs: 3072, Bias: -23.266652, T: 4800000, Avg. loss: 907.069404\n",
      "Total training time: 42.60 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 175.56, NNZs: 3072, Bias: -14.022853, T: 4800000, Avg. loss: 916.118514\n",
      "Total training time: 42.60 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 191.80, NNZs: 3072, Bias: -13.416919, T: 5000000, Avg. loss: 700.425260\n",
      "Total training time: 42.82 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 195.75, NNZs: 3072, Bias: -106.225043, T: 4900000, Avg. loss: 1015.372700\n",
      "Total training time: 43.06 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 169.94, NNZs: 3072, Bias: -23.277733, T: 4850000, Avg. loss: 900.023515\n",
      "Total training time: 43.09 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 174.66, NNZs: 3072, Bias: -14.038025, T: 4850000, Avg. loss: 902.724264\n",
      "Total training time: 43.10 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 191.13, NNZs: 3072, Bias: -13.431704, T: 5050000, Avg. loss: 686.927905\n",
      "Total training time: 43.28 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 194.61, NNZs: 3072, Bias: -106.327227, T: 4950000, Avg. loss: 1026.807470\n",
      "Total training time: 43.50 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 168.94, NNZs: 3072, Bias: -23.296564, T: 4900000, Avg. loss: 886.011453\n",
      "Total training time: 43.54 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 173.77, NNZs: 3072, Bias: -14.051324, T: 4900000, Avg. loss: 893.056523\n",
      "Total training time: 43.56 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 190.23, NNZs: 3072, Bias: -13.444920, T: 5100000, Avg. loss: 686.478917\n",
      "Total training time: 43.73 seconds.\n",
      "-- Epoch 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 193.99, NNZs: 3072, Bias: -106.428759, T: 5000000, Avg. loss: 1007.752761\n",
      "Total training time: 43.96 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 168.24, NNZs: 3072, Bias: -23.312066, T: 4950000, Avg. loss: 877.118922\n",
      "Total training time: 44.01 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 172.86, NNZs: 3072, Bias: -14.066595, T: 4950000, Avg. loss: 878.800447\n",
      "Total training time: 44.03 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 189.49, NNZs: 3072, Bias: -13.462010, T: 5150000, Avg. loss: 678.478667\n",
      "Total training time: 44.18 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 192.83, NNZs: 3072, Bias: -106.529914, T: 5050000, Avg. loss: 990.197780\n",
      "Total training time: 44.41 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 167.26, NNZs: 3072, Bias: -23.328247, T: 5000000, Avg. loss: 879.400998\n",
      "Total training time: 44.46 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 172.05, NNZs: 3072, Bias: -14.086435, T: 5000000, Avg. loss: 871.883230\n",
      "Total training time: 44.46 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 188.74, NNZs: 3072, Bias: -13.478468, T: 5200000, Avg. loss: 676.958697\n",
      "Total training time: 44.61 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 192.07, NNZs: 3072, Bias: -106.628696, T: 5100000, Avg. loss: 980.099358\n",
      "Total training time: 44.87 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 166.39, NNZs: 3072, Bias: -23.344768, T: 5050000, Avg. loss: 862.225255\n",
      "Total training time: 44.92 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 171.29, NNZs: 3072, Bias: -14.096361, T: 5050000, Avg. loss: 869.786843\n",
      "Total training time: 44.93 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 188.00, NNZs: 3072, Bias: -13.492212, T: 5250000, Avg. loss: 668.438664\n",
      "Total training time: 45.04 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 190.93, NNZs: 3072, Bias: -106.730437, T: 5150000, Avg. loss: 977.777272\n",
      "Total training time: 45.31 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 165.71, NNZs: 3072, Bias: -23.354530, T: 5100000, Avg. loss: 843.602809\n",
      "Total training time: 45.38 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 170.49, NNZs: 3072, Bias: -14.108987, T: 5100000, Avg. loss: 859.082464\n",
      "Total training time: 45.39 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 187.18, NNZs: 3072, Bias: -13.504345, T: 5300000, Avg. loss: 658.214371\n",
      "Total training time: 45.50 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 189.88, NNZs: 3072, Bias: -106.827059, T: 5200000, Avg. loss: 963.237154\n",
      "Total training time: 45.80 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 165.01, NNZs: 3072, Bias: -23.367557, T: 5150000, Avg. loss: 840.120643\n",
      "Total training time: 45.87 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 169.45, NNZs: 3072, Bias: -14.122944, T: 5150000, Avg. loss: 857.625097\n",
      "Total training time: 45.89 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 186.35, NNZs: 3072, Bias: -13.519506, T: 5350000, Avg. loss: 656.374702\n",
      "Total training time: 45.95 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 188.79, NNZs: 3072, Bias: -106.926438, T: 5250000, Avg. loss: 951.208592\n",
      "Total training time: 46.24 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 163.99, NNZs: 3072, Bias: -23.383595, T: 5200000, Avg. loss: 837.205737\n",
      "Total training time: 46.31 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 168.56, NNZs: 3072, Bias: -14.135796, T: 5200000, Avg. loss: 847.577327\n",
      "Total training time: 46.32 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 185.39, NNZs: 3072, Bias: -13.532636, T: 5400000, Avg. loss: 646.988638\n",
      "Total training time: 46.38 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 187.77, NNZs: 3072, Bias: -107.020471, T: 5300000, Avg. loss: 944.109573\n",
      "Total training time: 46.66 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 163.12, NNZs: 3072, Bias: -23.398648, T: 5250000, Avg. loss: 825.632610\n",
      "Total training time: 46.75 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 167.72, NNZs: 3072, Bias: -14.153506, T: 5250000, Avg. loss: 829.159690\n",
      "Total training time: 46.76 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 184.34, NNZs: 3072, Bias: -13.546842, T: 5450000, Avg. loss: 633.422562\n",
      "Total training time: 46.82 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 186.73, NNZs: 3072, Bias: -107.113512, T: 5350000, Avg. loss: 928.677578\n",
      "Total training time: 47.12 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 162.62, NNZs: 3072, Bias: -23.414200, T: 5300000, Avg. loss: 815.653083\n",
      "Total training time: 47.22 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 166.96, NNZs: 3072, Bias: -14.168561, T: 5300000, Avg. loss: 815.100460\n",
      "Total training time: 47.23 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 183.95, NNZs: 3072, Bias: -13.561835, T: 5500000, Avg. loss: 634.604254\n",
      "Total training time: 47.28 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 185.90, NNZs: 3072, Bias: -107.209206, T: 5400000, Avg. loss: 932.194592\n",
      "Total training time: 47.57 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 166.38, NNZs: 3072, Bias: -14.179689, T: 5350000, Avg. loss: 814.729926\n",
      "Total training time: 47.69 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 161.82, NNZs: 3072, Bias: -23.423456, T: 5350000, Avg. loss: 819.742036\n",
      "Total training time: 47.69 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 183.04, NNZs: 3072, Bias: -13.572928, T: 5550000, Avg. loss: 623.424248\n",
      "Total training time: 47.73 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 185.08, NNZs: 3072, Bias: -107.299783, T: 5450000, Avg. loss: 922.434228\n",
      "Total training time: 48.07 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 165.51, NNZs: 3072, Bias: -14.195625, T: 5400000, Avg. loss: 809.788338\n",
      "Total training time: 48.18 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 160.87, NNZs: 3072, Bias: -23.439696, T: 5400000, Avg. loss: 807.088071\n",
      "Total training time: 48.18 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 182.21, NNZs: 3072, Bias: -13.587102, T: 5600000, Avg. loss: 618.999348\n",
      "Total training time: 48.20 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 184.13, NNZs: 3072, Bias: -107.396110, T: 5500000, Avg. loss: 917.320968\n",
      "Total training time: 48.51 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 164.89, NNZs: 3072, Bias: -14.212064, T: 5450000, Avg. loss: 803.044346\n",
      "Total training time: 48.63 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 181.43, NNZs: 3072, Bias: -13.597772, T: 5650000, Avg. loss: 617.721532\n",
      "Total training time: 48.63 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 160.18, NNZs: 3072, Bias: -23.455627, T: 5450000, Avg. loss: 804.618368\n",
      "Total training time: 48.63 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 183.38, NNZs: 3072, Bias: -107.484801, T: 5550000, Avg. loss: 900.123467\n",
      "Total training time: 48.96 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 180.73, NNZs: 3072, Bias: -13.610083, T: 5700000, Avg. loss: 615.270494\n",
      "Total training time: 49.07 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 164.27, NNZs: 3072, Bias: -14.225279, T: 5500000, Avg. loss: 795.424580\n",
      "Total training time: 49.08 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 159.41, NNZs: 3072, Bias: -23.469981, T: 5500000, Avg. loss: 792.523951\n",
      "Total training time: 49.09 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 182.73, NNZs: 3072, Bias: -107.575681, T: 5600000, Avg. loss: 898.857738\n",
      "Total training time: 49.40 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 180.07, NNZs: 3072, Bias: -13.623455, T: 5750000, Avg. loss: 606.409631\n",
      "Total training time: 49.50 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 163.52, NNZs: 3072, Bias: -14.242389, T: 5550000, Avg. loss: 782.900841\n",
      "Total training time: 49.52 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 158.80, NNZs: 3072, Bias: -23.482537, T: 5550000, Avg. loss: 779.425359\n",
      "Total training time: 49.53 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 182.00, NNZs: 3072, Bias: -107.665281, T: 5650000, Avg. loss: 881.257039\n",
      "Total training time: 49.85 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 179.55, NNZs: 3072, Bias: -13.638521, T: 5800000, Avg. loss: 592.818818\n",
      "Total training time: 49.94 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 163.04, NNZs: 3072, Bias: -14.256859, T: 5600000, Avg. loss: 774.364352\n",
      "Total training time: 49.97 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 158.01, NNZs: 3072, Bias: -23.500795, T: 5600000, Avg. loss: 779.119755\n",
      "Total training time: 49.98 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 181.06, NNZs: 3072, Bias: -107.760497, T: 5700000, Avg. loss: 889.250839\n",
      "Total training time: 50.28 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 178.67, NNZs: 3072, Bias: -13.652713, T: 5850000, Avg. loss: 598.946079\n",
      "Total training time: 50.37 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 162.25, NNZs: 3072, Bias: -14.270510, T: 5650000, Avg. loss: 781.633890\n",
      "Total training time: 50.42 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 157.31, NNZs: 3072, Bias: -23.512890, T: 5650000, Avg. loss: 773.752821\n",
      "Total training time: 50.44 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 180.08, NNZs: 3072, Bias: -107.850981, T: 5750000, Avg. loss: 876.751547\n",
      "Total training time: 50.74 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 177.84, NNZs: 3072, Bias: -13.668513, T: 5900000, Avg. loss: 595.907029\n",
      "Total training time: 50.83 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 161.45, NNZs: 3072, Bias: -14.286529, T: 5700000, Avg. loss: 762.465479\n",
      "Total training time: 50.89 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 156.65, NNZs: 3072, Bias: -23.529661, T: 5700000, Avg. loss: 764.156677\n",
      "Total training time: 50.90 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 179.23, NNZs: 3072, Bias: -107.939885, T: 5800000, Avg. loss: 859.877749\n",
      "Total training time: 51.17 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 177.06, NNZs: 3072, Bias: -13.681875, T: 5950000, Avg. loss: 594.018402\n",
      "Total training time: 51.26 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 160.76, NNZs: 3072, Bias: -14.303153, T: 5750000, Avg. loss: 759.361925\n",
      "Total training time: 51.33 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 156.12, NNZs: 3072, Bias: -23.541899, T: 5750000, Avg. loss: 763.166243\n",
      "Total training time: 51.34 seconds.\n",
      "-- Epoch 116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 178.52, NNZs: 3072, Bias: -108.026800, T: 5850000, Avg. loss: 862.916425\n",
      "Total training time: 51.61 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 176.34, NNZs: 3072, Bias: -13.697522, T: 6000000, Avg. loss: 580.263855\n",
      "Total training time: 51.67 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 159.90, NNZs: 3072, Bias: -14.316942, T: 5800000, Avg. loss: 749.558959\n",
      "Total training time: 51.78 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 155.37, NNZs: 3072, Bias: -23.556200, T: 5800000, Avg. loss: 750.060347\n",
      "Total training time: 51.79 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 178.06, NNZs: 3072, Bias: -108.112037, T: 5900000, Avg. loss: 857.229445\n",
      "Total training time: 52.05 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 175.75, NNZs: 3072, Bias: -13.710901, T: 6050000, Avg. loss: 579.111543\n",
      "Total training time: 52.11 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 159.29, NNZs: 3072, Bias: -14.331308, T: 5850000, Avg. loss: 737.662375\n",
      "Total training time: 52.23 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 154.49, NNZs: 3072, Bias: -23.571733, T: 5850000, Avg. loss: 742.057108\n",
      "Total training time: 52.24 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 177.12, NNZs: 3072, Bias: -108.195800, T: 5950000, Avg. loss: 844.833163\n",
      "Total training time: 52.49 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 175.09, NNZs: 3072, Bias: -13.721589, T: 6100000, Avg. loss: 578.655532\n",
      "Total training time: 52.55 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 158.28, NNZs: 3072, Bias: -14.345615, T: 5900000, Avg. loss: 737.639469\n",
      "Total training time: 52.68 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 154.05, NNZs: 3072, Bias: -23.582443, T: 5900000, Avg. loss: 726.764266\n",
      "Total training time: 52.69 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 176.51, NNZs: 3072, Bias: -108.281954, T: 6000000, Avg. loss: 821.178479\n",
      "Total training time: 52.94 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 174.21, NNZs: 3072, Bias: -13.736595, T: 6150000, Avg. loss: 569.627410\n",
      "Total training time: 52.99 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 157.67, NNZs: 3072, Bias: -14.360638, T: 5950000, Avg. loss: 731.349844\n",
      "Total training time: 53.13 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 153.32, NNZs: 3072, Bias: -23.597723, T: 5950000, Avg. loss: 739.438906\n",
      "Total training time: 53.14 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 175.88, NNZs: 3072, Bias: -108.369623, T: 6050000, Avg. loss: 841.091139\n",
      "Total training time: 53.37 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 173.25, NNZs: 3072, Bias: -13.747161, T: 6200000, Avg. loss: 563.172667\n",
      "Total training time: 53.42 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 156.84, NNZs: 3072, Bias: -14.377404, T: 6000000, Avg. loss: 731.350977\n",
      "Total training time: 53.60 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 152.70, NNZs: 3072, Bias: -23.609210, T: 6000000, Avg. loss: 727.279082\n",
      "Total training time: 53.62 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 174.92, NNZs: 3072, Bias: -108.456019, T: 6100000, Avg. loss: 817.756375\n",
      "Total training time: 53.85 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 172.42, NNZs: 3072, Bias: -13.759723, T: 6250000, Avg. loss: 559.544148\n",
      "Total training time: 53.89 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 156.29, NNZs: 3072, Bias: -14.386919, T: 6050000, Avg. loss: 712.319654\n",
      "Total training time: 54.08 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 152.12, NNZs: 3072, Bias: -23.624450, T: 6050000, Avg. loss: 714.165497\n",
      "Total training time: 54.10 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 174.22, NNZs: 3072, Bias: -108.539884, T: 6150000, Avg. loss: 809.636873\n",
      "Total training time: 54.29 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 171.59, NNZs: 3072, Bias: -13.774030, T: 6300000, Avg. loss: 560.034834\n",
      "Total training time: 54.31 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 155.68, NNZs: 3072, Bias: -14.397487, T: 6100000, Avg. loss: 717.868384\n",
      "Total training time: 54.52 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 151.55, NNZs: 3072, Bias: -23.640690, T: 6100000, Avg. loss: 709.566697\n",
      "Total training time: 54.53 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 173.40, NNZs: 3072, Bias: -108.619506, T: 6200000, Avg. loss: 809.322827\n",
      "Total training time: 54.74 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 170.96, NNZs: 3072, Bias: -13.785970, T: 6350000, Avg. loss: 546.283854\n",
      "Total training time: 54.76 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 155.05, NNZs: 3072, Bias: -14.410131, T: 6150000, Avg. loss: 708.080799\n",
      "Total training time: 55.00 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 150.84, NNZs: 3072, Bias: -23.656235, T: 6150000, Avg. loss: 706.372280\n",
      "Total training time: 55.01 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 172.82, NNZs: 3072, Bias: -108.699125, T: 6250000, Avg. loss: 801.979917\n",
      "Total training time: 55.19 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 170.33, NNZs: 3072, Bias: -13.802108, T: 6400000, Avg. loss: 545.632877\n",
      "Total training time: 55.20 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 154.31, NNZs: 3072, Bias: -14.418270, T: 6200000, Avg. loss: 713.219869\n",
      "Total training time: 55.46 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 150.05, NNZs: 3072, Bias: -23.668248, T: 6200000, Avg. loss: 699.018304\n",
      "Total training time: 55.46 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 172.07, NNZs: 3072, Bias: -108.781292, T: 6300000, Avg. loss: 790.479941\n",
      "Total training time: 55.63 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 169.79, NNZs: 3072, Bias: -13.812072, T: 6450000, Avg. loss: 534.890566\n",
      "Total training time: 55.64 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 149.39, NNZs: 3072, Bias: -23.680317, T: 6250000, Avg. loss: 702.548162\n",
      "Total training time: 55.93 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 153.62, NNZs: 3072, Bias: -14.428814, T: 6250000, Avg. loss: 696.784918\n",
      "Total training time: 55.94 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 169.03, NNZs: 3072, Bias: -13.825335, T: 6500000, Avg. loss: 532.531166\n",
      "Total training time: 56.08 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 171.31, NNZs: 3072, Bias: -108.860806, T: 6350000, Avg. loss: 790.099364\n",
      "Total training time: 56.09 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 153.12, NNZs: 3072, Bias: -14.444431, T: 6300000, Avg. loss: 689.974894\n",
      "Total training time: 56.37 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 148.74, NNZs: 3072, Bias: -23.697351, T: 6300000, Avg. loss: 683.670143\n",
      "Total training time: 56.37 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 168.50, NNZs: 3072, Bias: -13.838704, T: 6550000, Avg. loss: 528.853440\n",
      "Total training time: 56.50 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 170.73, NNZs: 3072, Bias: -108.941294, T: 6400000, Avg. loss: 775.348698\n",
      "Total training time: 56.51 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 152.49, NNZs: 3072, Bias: -14.456714, T: 6350000, Avg. loss: 686.248712\n",
      "Total training time: 56.83 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 148.13, NNZs: 3072, Bias: -23.707185, T: 6350000, Avg. loss: 683.412739\n",
      "Total training time: 56.83 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 167.72, NNZs: 3072, Bias: -13.853923, T: 6600000, Avg. loss: 524.395838\n",
      "Total training time: 56.95 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 170.09, NNZs: 3072, Bias: -109.024150, T: 6450000, Avg. loss: 782.848149\n",
      "Total training time: 56.95 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 151.79, NNZs: 3072, Bias: -14.468661, T: 6400000, Avg. loss: 677.915969\n",
      "Total training time: 57.26 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 147.60, NNZs: 3072, Bias: -23.719958, T: 6400000, Avg. loss: 681.056809\n",
      "Total training time: 57.26 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 167.10, NNZs: 3072, Bias: -13.865310, T: 6650000, Avg. loss: 518.081693\n",
      "Total training time: 57.37 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 169.48, NNZs: 3072, Bias: -109.104076, T: 6500000, Avg. loss: 776.788335\n",
      "Total training time: 57.38 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 151.32, NNZs: 3072, Bias: -14.480973, T: 6450000, Avg. loss: 671.732096\n",
      "Total training time: 57.69 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 146.99, NNZs: 3072, Bias: -23.733328, T: 6450000, Avg. loss: 674.764518\n",
      "Total training time: 57.70 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 166.59, NNZs: 3072, Bias: -13.878217, T: 6700000, Avg. loss: 514.228200\n",
      "Total training time: 57.81 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 168.79, NNZs: 3072, Bias: -109.182323, T: 6550000, Avg. loss: 758.085751\n",
      "Total training time: 57.82 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 150.62, NNZs: 3072, Bias: -14.489502, T: 6500000, Avg. loss: 667.761438\n",
      "Total training time: 58.18 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 146.42, NNZs: 3072, Bias: -23.746532, T: 6500000, Avg. loss: 673.898764\n",
      "Total training time: 58.19 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 165.85, NNZs: 3072, Bias: -13.891919, T: 6750000, Avg. loss: 513.473885\n",
      "Total training time: 58.28 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 168.12, NNZs: 3072, Bias: -109.262711, T: 6600000, Avg. loss: 765.026728\n",
      "Total training time: 58.30 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 149.92, NNZs: 3072, Bias: -14.502296, T: 6550000, Avg. loss: 664.074570\n",
      "Total training time: 58.62 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 145.68, NNZs: 3072, Bias: -23.758845, T: 6550000, Avg. loss: 658.996170\n",
      "Total training time: 58.63 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 165.08, NNZs: 3072, Bias: -13.904782, T: 6800000, Avg. loss: 515.118149\n",
      "Total training time: 58.69 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 167.57, NNZs: 3072, Bias: -109.339350, T: 6650000, Avg. loss: 748.797433\n",
      "Total training time: 58.73 seconds.\n",
      "-- Epoch 134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 149.50, NNZs: 3072, Bias: -14.516556, T: 6600000, Avg. loss: 667.189250\n",
      "Total training time: 59.07 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 145.06, NNZs: 3072, Bias: -23.772696, T: 6600000, Avg. loss: 666.288520\n",
      "Total training time: 59.09 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 164.48, NNZs: 3072, Bias: -13.917586, T: 6850000, Avg. loss: 509.668585\n",
      "Total training time: 59.13 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 166.83, NNZs: 3072, Bias: -109.417844, T: 6700000, Avg. loss: 747.398363\n",
      "Total training time: 59.17 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 149.12, NNZs: 3072, Bias: -14.528035, T: 6650000, Avg. loss: 657.064799\n",
      "Total training time: 59.52 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 144.69, NNZs: 3072, Bias: -23.785710, T: 6650000, Avg. loss: 650.882710\n",
      "Total training time: 59.54 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 163.81, NNZs: 3072, Bias: -13.929947, T: 6900000, Avg. loss: 507.002551\n",
      "Total training time: 59.56 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 166.33, NNZs: 3072, Bias: -109.493712, T: 6750000, Avg. loss: 737.079602\n",
      "Total training time: 59.63 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 148.48, NNZs: 3072, Bias: -14.537042, T: 6700000, Avg. loss: 650.754026\n",
      "Total training time: 59.99 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 163.33, NNZs: 3072, Bias: -13.941479, T: 6950000, Avg. loss: 496.859054\n",
      "Total training time: 60.01 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 144.12, NNZs: 3072, Bias: -23.800135, T: 6700000, Avg. loss: 649.923709\n",
      "Total training time: 60.00 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 165.77, NNZs: 3072, Bias: -109.565784, T: 6800000, Avg. loss: 732.588147\n",
      "Total training time: 60.08 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 162.85, NNZs: 3072, Bias: -13.952980, T: 7000000, Avg. loss: 494.590564\n",
      "Total training time: 60.43 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 147.84, NNZs: 3072, Bias: -14.547686, T: 6750000, Avg. loss: 650.858659\n",
      "Total training time: 60.43 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 143.57, NNZs: 3072, Bias: -23.810646, T: 6750000, Avg. loss: 645.764248\n",
      "Total training time: 60.44 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 164.91, NNZs: 3072, Bias: -109.643326, T: 6850000, Avg. loss: 737.100210\n",
      "Total training time: 60.49 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 162.20, NNZs: 3072, Bias: -13.962392, T: 7050000, Avg. loss: 495.640172\n",
      "Total training time: 60.84 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 147.26, NNZs: 3072, Bias: -14.559724, T: 6800000, Avg. loss: 636.278149\n",
      "Total training time: 60.87 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 143.09, NNZs: 3072, Bias: -23.818234, T: 6800000, Avg. loss: 633.355333\n",
      "Total training time: 60.88 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 164.38, NNZs: 3072, Bias: -109.721882, T: 6900000, Avg. loss: 729.719629\n",
      "Total training time: 60.94 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 161.57, NNZs: 3072, Bias: -13.974293, T: 7100000, Avg. loss: 489.977118\n",
      "Total training time: 61.26 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 146.75, NNZs: 3072, Bias: -14.566436, T: 6850000, Avg. loss: 633.151303\n",
      "Total training time: 61.30 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 142.52, NNZs: 3072, Bias: -23.826658, T: 6850000, Avg. loss: 635.361235\n",
      "Total training time: 61.33 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 163.82, NNZs: 3072, Bias: -109.795058, T: 6950000, Avg. loss: 716.157398\n",
      "Total training time: 61.37 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 161.01, NNZs: 3072, Bias: -13.985977, T: 7150000, Avg. loss: 482.310004\n",
      "Total training time: 61.67 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 146.36, NNZs: 3072, Bias: -14.574826, T: 6900000, Avg. loss: 636.495365\n",
      "Total training time: 61.73 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 141.89, NNZs: 3072, Bias: -23.840141, T: 6900000, Avg. loss: 627.153624\n",
      "Total training time: 61.75 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 163.33, NNZs: 3072, Bias: -109.870587, T: 7000000, Avg. loss: 722.367614\n",
      "Total training time: 61.79 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 160.46, NNZs: 3072, Bias: -13.995754, T: 7200000, Avg. loss: 484.766873\n",
      "Total training time: 62.09 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 145.78, NNZs: 3072, Bias: -14.587954, T: 6950000, Avg. loss: 627.115347\n",
      "Total training time: 62.16 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 141.25, NNZs: 3072, Bias: -23.854455, T: 6950000, Avg. loss: 628.477645\n",
      "Total training time: 62.18 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 162.81, NNZs: 3072, Bias: -109.943295, T: 7050000, Avg. loss: 707.731463\n",
      "Total training time: 62.22 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 159.95, NNZs: 3072, Bias: -14.005489, T: 7250000, Avg. loss: 476.834935\n",
      "Total training time: 62.49 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 145.22, NNZs: 3072, Bias: -14.599014, T: 7000000, Avg. loss: 626.951025\n",
      "Total training time: 62.59 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 140.83, NNZs: 3072, Bias: -23.863417, T: 7000000, Avg. loss: 624.404245\n",
      "Total training time: 62.61 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 162.45, NNZs: 3072, Bias: -110.013992, T: 7100000, Avg. loss: 706.832546\n",
      "Total training time: 62.63 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 159.29, NNZs: 3072, Bias: -14.017249, T: 7300000, Avg. loss: 485.188848\n",
      "Total training time: 62.91 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 144.72, NNZs: 3072, Bias: -14.612072, T: 7050000, Avg. loss: 617.402911\n",
      "Total training time: 63.03 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 140.36, NNZs: 3072, Bias: -23.873765, T: 7050000, Avg. loss: 618.370283\n",
      "Total training time: 63.05 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 161.87, NNZs: 3072, Bias: -110.086838, T: 7150000, Avg. loss: 705.370858\n",
      "Total training time: 63.07 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 158.78, NNZs: 3072, Bias: -14.028218, T: 7350000, Avg. loss: 471.997633\n",
      "Total training time: 63.34 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 144.31, NNZs: 3072, Bias: -14.620501, T: 7100000, Avg. loss: 614.305594\n",
      "Total training time: 63.48 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 139.84, NNZs: 3072, Bias: -23.886733, T: 7100000, Avg. loss: 605.440104\n",
      "Total training time: 63.49 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 161.34, NNZs: 3072, Bias: -110.160366, T: 7200000, Avg. loss: 697.010152\n",
      "Total training time: 63.49 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 158.23, NNZs: 3072, Bias: -14.037741, T: 7400000, Avg. loss: 471.357864\n",
      "Total training time: 63.77 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 143.78, NNZs: 3072, Bias: -14.634533, T: 7150000, Avg. loss: 604.921530\n",
      "Total training time: 63.93 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 160.73, NNZs: 3072, Bias: -110.233033, T: 7250000, Avg. loss: 687.807219\n",
      "Total training time: 63.93 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 139.42, NNZs: 3072, Bias: -23.898883, T: 7150000, Avg. loss: 604.245395\n",
      "Total training time: 63.94 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 157.76, NNZs: 3072, Bias: -14.050578, T: 7450000, Avg. loss: 466.396346\n",
      "Total training time: 64.20 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 160.19, NNZs: 3072, Bias: -110.304872, T: 7300000, Avg. loss: 689.142302\n",
      "Total training time: 64.36 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 143.33, NNZs: 3072, Bias: -14.644765, T: 7200000, Avg. loss: 608.024111\n",
      "Total training time: 64.37 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 138.96, NNZs: 3072, Bias: -23.908223, T: 7200000, Avg. loss: 606.085800\n",
      "Total training time: 64.37 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 157.14, NNZs: 3072, Bias: -14.062353, T: 7500000, Avg. loss: 457.252806\n",
      "Total training time: 64.60 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 159.55, NNZs: 3072, Bias: -110.378580, T: 7350000, Avg. loss: 678.236032\n",
      "Total training time: 64.79 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 142.83, NNZs: 3072, Bias: -14.655539, T: 7250000, Avg. loss: 602.933127\n",
      "Total training time: 64.81 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 138.64, NNZs: 3072, Bias: -23.917307, T: 7250000, Avg. loss: 597.269968\n",
      "Total training time: 64.82 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 156.61, NNZs: 3072, Bias: -14.074918, T: 7550000, Avg. loss: 459.433159\n",
      "Total training time: 65.04 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 158.98, NNZs: 3072, Bias: -110.448870, T: 7400000, Avg. loss: 682.416084\n",
      "Total training time: 65.24 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 142.36, NNZs: 3072, Bias: -14.670609, T: 7300000, Avg. loss: 598.879921\n",
      "Total training time: 65.27 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 138.16, NNZs: 3072, Bias: -23.928798, T: 7300000, Avg. loss: 598.249397\n",
      "Total training time: 65.27 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 156.07, NNZs: 3072, Bias: -14.083261, T: 7600000, Avg. loss: 457.991689\n",
      "Total training time: 65.46 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 158.53, NNZs: 3072, Bias: -110.518049, T: 7450000, Avg. loss: 658.387435\n",
      "Total training time: 65.69 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 141.57, NNZs: 3072, Bias: -14.684947, T: 7350000, Avg. loss: 592.674728\n",
      "Total training time: 65.72 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 137.67, NNZs: 3072, Bias: -23.935375, T: 7350000, Avg. loss: 596.025035\n",
      "Total training time: 65.72 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 155.61, NNZs: 3072, Bias: -14.094577, T: 7650000, Avg. loss: 457.358037\n",
      "Total training time: 65.91 seconds.\n",
      "-- Epoch 154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 157.95, NNZs: 3072, Bias: -110.586002, T: 7500000, Avg. loss: 669.550088\n",
      "Total training time: 66.14 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 141.04, NNZs: 3072, Bias: -14.695599, T: 7400000, Avg. loss: 589.978783\n",
      "Total training time: 66.18 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 137.15, NNZs: 3072, Bias: -23.949168, T: 7400000, Avg. loss: 579.264530\n",
      "Total training time: 66.18 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 155.23, NNZs: 3072, Bias: -14.104745, T: 7700000, Avg. loss: 452.677244\n",
      "Total training time: 66.35 seconds.\n",
      "-- Epoch 155\n"
     ]
    }
   ],
   "source": [
    "sgdlr.fit(X,Y)\n",
    "score = sgdlr.score(test_X,test_Y)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SVM Regression Model with Stochastic Gradient Descent \n",
    "X,Y = load_cfar10_batch(\"data/\",1,False)\n",
    "\n",
    "for n in range(2,6):\n",
    "    x,y = load_cfar10_batch(\"data/\",n,False)\n",
    "    X = np.concatenate((X,x),axis=0)\n",
    "    Y = np.concatenate((Y,y),axis=0)\n",
    "\n",
    "test_X,test_Y = load_cfar10_batch(\"data/test_batch\",None,False)\n",
    "#labelnames = unpickle(\"data/batches.meta\")\n",
    "\n",
    "sgd_svm = SGDClassifier(alpha=0.001, max_iter=10000, tol=1e-3, verbose = 1,n_jobs=4)\n",
    "\n",
    "print(\"Data shape: \",X.shape)\n",
    "print(\"Labels shape: \",Y.shape)\n",
    "\n",
    "print(\"Test Data shape: \",test_X.shape)\n",
    "print(\"Test Labels shape: \",test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_svm.fit(X,Y)\n",
    "svmscore = sgd_svm.score(test_X,test_Y)\n",
    "print(svmscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Advantages\n",
    "\n",
    "In this section we will discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Feature Importance\n",
    "\n",
    "In this section we will use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Support Vectors\n",
    "\n",
    "In this section we will look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
